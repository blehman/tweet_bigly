{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set PATH\n",
    "<pre>\n",
    "$ PATH=$PATH: &lt;pwd&gt;\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the environment\n",
    "<pre>\n",
    "$conda env create -f environment.yml\n",
    "$source activate tweet_bigly_env\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Jupyter Notebook\n",
    "    $jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data\n",
    "The NYTimes hosts [this article](https://www.nytimes.com/interactive/2016/01/28/upshot/donald-trump-twitter-insults.html) with Tweet content that they have identified as insults. Our goal is to obtain a well formated list containg the following fields:\n",
    "<pre>\n",
    "{\n",
    "    \"group\": (string - defined category),\n",
    "    \"date\": (string - Trump Tweet date),\n",
    "    \"link\": (string - link to Tweet),\n",
    "    \"body\": (string - insult),\n",
    "    \"name\": (string - name of insultee),\n",
    "    \"title\": (string - title of insultee) \n",
    "}\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import configparser\n",
    "#config = configparser.ConfigParser()\n",
    "#config.read('config.cfg')\n",
    "import requests\n",
    "#import time\n",
    "import pickle\n",
    "#from collections import Counter\n",
    "#from dateutil.parser import parse as dateutil_parse\n",
    "#import dateutil\n",
    "#import pandas as pd\n",
    "#from IPython.display import display\n",
    "import ujson as json\n",
    "from datetime import datetime\n",
    "#import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review site\n",
    "Look at the html in [this article](https://www.nytimes.com/interactive/2016/01/28/upshot/donald-trump-twitter-insults.html) and try to understand the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from sklearn.externals import joblib\n",
    "import pickle\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are there no tweet links? What happened? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = 'https://www.nytimes.com/interactive/2016/01/28/upshot/donald-trump-twitter-insults.html'\n",
    "today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "file_name = 'data/'+today+'_test_page.html'\n",
    "page = requests.get(url)\n",
    "#print(page.text)\n",
    "#headers = {'Accept-Encoding': 'identity'}\n",
    "#r = requests.get(url, headers=headers)\n",
    "#print(r)\n",
    "#tree = html.fromstring(page.content)\n",
    "data = page.text\n",
    "\n",
    "with open(file_name,'w') as f:\n",
    "    f.write(data)\n",
    "    \n",
    "soup = BeautifulSoup(data, \"lxml\")\n",
    "\n",
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://www.nytimes.com/interactive/2016/01/28/upshot/donald-trump-twitter-insults.html'\n",
    "today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "browser = webdriver.Chrome()\n",
    "browser.get(url)\n",
    "html = browser.page_source #print this and compare the difference to the page text from above\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "tweets = []\n",
    "for link in soup.find_all('a'):\n",
    "    href = link.get('href')\n",
    "    if href==None:\n",
    "        continue\n",
    "    elif href.startswith('https://twitter.com/realDonaldTrump/status/'):\n",
    "        tweets.append(href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the Tweet links, how we can search for the other information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html,\"lxml\")\n",
    "tweets = []\n",
    "for a in soup.select('.g-insult-links-c a[href^=\"https://twitter.com/realDonaldTrump/status/\"]'):\n",
    "    name = a.parent.parent.parent.select('.g-entity-name')[0].string\n",
    "    title = a.parent.parent.parent.select('.g-entity-title')[0].string\n",
    "    link = a.attrs['href']\n",
    "    text = a.string[1:-1] #removing added quotes\n",
    "    date = a.next_sibling.string\n",
    "    tweets.append({\"name\":name\n",
    "                   ,\"title\":title\n",
    "                   ,\"link\":link\n",
    "                   ,\"body\":text\n",
    "                   ,\"date\":date})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets[-3:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Curation\n",
    "Self identify clusters and save to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters = [\"US Business - financially focused individuals and companies\"\n",
    "            , \"US Political - senator, governor, mayor, democrats, republicans, related rant\"\n",
    "            , \"Foreign Interest - person, country, related topic\"\n",
    "            , \"Random - inanimate objects, golf courses, sporting events, books\"\n",
    "            , \"Famous - people, broadway shows, tv shows, popular songs\"\n",
    "            , \"News - person, org, allegations, association\"\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The color groups file was created to allow manual addition of group labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groups = set()\n",
    "for d in tweets:\n",
    "    title=d['title']\n",
    "    if not d['title']:\n",
    "        title = ''\n",
    "    groups.add((d[\"name\"]+'|'+title))\n",
    "    \n",
    "with open('data/{}_color_groups.json'.format(today),'w') as f:\n",
    "    for item in groups:\n",
    "        rec = item.split(\"|\")\n",
    "        d = {\"DELETE\":item\n",
    "             ,\"name\":rec[0]\n",
    "             ,\"group\":\"\"\n",
    "             }\n",
    "        f.write(json.dumps(d)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the color_groups file was edited, the next step was to insert this new information into the tweet list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create group dict keyed on name\n",
    "name_group_dict ={}\n",
    "with open(\"data/2017-01-27_color_groups.json\",'r') as f:\n",
    "    for item in f:\n",
    "        rec = json.loads(item)\n",
    "        name_group_dict[rec[\"name\"]]=rec[\"group\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for item in tweets:\n",
    "    item['group']=name_group_dict [item['name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.setrecursionlimit(90000) ##for potention recursion depth error\n",
    "with open(\"data/{}_full_insult_list.json\".format(today),'w') as f, open(\"data/{}_full_insult_list.json.pkl\".format(today),'wb') as pkl:\n",
    "    f.write(json.dumps(tweets))\n",
    "    pickle.dump(tweets,pkl, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO D3 \n",
    "If you want to take a gander: [d3js.org](https://d3js.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#...end"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tweet_bigly_env]",
   "language": "python",
   "name": "conda-env-tweet_bigly_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
